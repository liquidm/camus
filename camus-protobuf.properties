etl.destination.path=/history
etl.execution.base.path=/camus/running
etl.execution.history.path=/camus/state

camus.message.decoder.class=com.liquidm.camus.KafkaJSONProtobufDecoder
etl.record.writer.provider.class=com.liquidm.camus.ParquetCamusWriter
mapred.map.tasks=30

# max historical time that will be pulled from each partition based on event timestamp
kafka.max.pull.hrs=6
# events with a timestamp older than this will be discarded.
kafka.max.historical.days=10
# Max minutes for each mapper to pull messages (-1 means no limit)
kafka.max.pull.minutes.per.task=50

kafka.client.name=camus
# reading from cluster dw
kafka.brokers=144.76.113.73:9092,144.76.53.221:9092,144.76.46.155:9092,78.46.174.7:9092,78.46.174.245:9092,144.76.128.42:9092,144.76.115.101:9092
kafka.timeout.value=60000

# if whitelist has values, only whitelisted topic are pulled. if empty, everything is pulled
kafka.whitelist.topics=(ed_reports)

#Stops the mapper from getting inundated with Decoder exceptions for the same topic
#Default value is set to 10
max.decoder.exceptions.to.print=10

#Controls the submitting of counts to Kafka
post.tracking.counts.to.kafka=false

# everything below this point can be ignored for the time being, will provide more documentation down the road
##########################
etl.run.tracking.post=false
kafka.monitor.tier=
etl.counts.path=
kafka.monitor.time.granularity=10

etl.hourly=hourly
etl.daily=daily
etl.ignore.schema.errors=false

etl.default.timezone=UTC
etl.output.file.time.partition.mins=60
etl.keep.count.files=false
etl.execution.history.max.of.quota=.8

kafka.client.buffer.size=8388608
kafka.client.so.timeout=60000
