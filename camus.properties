etl.destination.path=~/tmp/history.development
etl.execution.base.path=~/tmp/camus.running
etl.execution.history.path=~/tmp/camus.state

camus.message.decoder.class=com.liquidm.camus.KafkaJSONMessageDecoder
etl.record.writer.provider.class=com.liquidm.camus.DruidWriter
mapred.map.tasks=30

# max historical time that will be pulled from each partition based on event timestamp
kafka.max.pull.hrs=6
# events with a timestamp older than this will be discarded.
kafka.max.historical.days=10
# Max minutes for each mapper to pull messages (-1 means no limit)
kafka.max.pull.minutes.per.task=1

kafka.client.name=camus-druid
# reading from cluster eu1
kafka.brokers=148.251.244.135:9092,148.251.244.165:9092,148.251.244.170:9092,148.251.244.171:9092,148.251.245.34:9092
kafka.timeout.value=60000

# if whitelist has values, only whitelisted topic are pulled. if empty, everything is pulled
kafka.whitelist.topics=ed_reports

#Stops the mapper from getting inundated with Decoder exceptions for the same topic
#Default value is set to 10
max.decoder.exceptions.to.print=10

#Controls the submitting of counts to Kafka
post.tracking.counts.to.kafka=false

# everything below this point can be ignored for the time being, will provide more documentation down the road
##########################
etl.run.tracking.post=false
kafka.monitor.tier=
etl.counts.path=
kafka.monitor.time.granularity=10

etl.hourly=hourly
etl.daily=daily
etl.ignore.schema.errors=false

etl.default.timezone=UTC
etl.output.file.time.partition.mins=60
etl.keep.count.files=false
etl.execution.history.max.of.quota=.8

kafka.client.buffer.size=8388608
kafka.client.so.timeout=60000
